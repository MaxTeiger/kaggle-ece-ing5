{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 64
    },
    "colab_type": "code",
    "id": "0yMQ61oGIquY",
    "outputId": "54cc84dd-6ce5-4e46-cacd-bc859f79e055"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import urllib\n",
    "import sys\n",
    "import os\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from keras_preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 126
    },
    "colab_type": "code",
    "id": "yB0DlJ18wIMg",
    "outputId": "bdc6a0d9-f787-40eb-81b2-9367dfe39a65"
   },
   "outputs": [],
   "source": [
    "\"\"\" Use the drive \"\"\"\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_UnT9awhh7g2"
   },
   "source": [
    "## Useful functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kdHimIPxixs7"
   },
   "outputs": [],
   "source": [
    "def unzip_single_file(zip_file_name, output_file_name):\n",
    "    \"\"\"\n",
    "        If the outFile is already created, don't recreate\n",
    "        If the outFile does not exist, create it from the zipFile\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(output_file_name):\n",
    "        with open(output_file_name, 'wb') as out_file:\n",
    "            with zipfile.ZipFile(zip_file_name) as zipped:\n",
    "                for info in zipped.infolist():\n",
    "                    if output_file_name in info.filename:\n",
    "                        with zipped.open(info) as requested_file:\n",
    "                            out_file.write(requested_file.read())\n",
    "                            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Gn9QFYQd9HW"
   },
   "outputs": [],
   "source": [
    "def create_glove_wordmap(glove_zip_file = \"glove.6B.zip\",\n",
    "                         glove_vectors_file = \"glove.6B.50d.txt\"\n",
    "                         ):\n",
    "  \n",
    "    from urllib.request import urlretrieve\n",
    "    #large file - 862 MB\n",
    "    if (not os.path.isfile(glove_zip_file) and\n",
    "        not os.path.isfile(glove_vectors_file)):\n",
    "        urlretrieve (\"http://nlp.stanford.edu/data/glove.6B.zip\", \n",
    "                    glove_zip_file)\n",
    "        \n",
    "    unzip_single_file(glove_zip_file, glove_vectors_file)\n",
    "\n",
    "    glove_wordmap = {}\n",
    "\n",
    "    with open(glove_vectors_file, \"r\") as glove:\n",
    "        for line in glove:\n",
    "            name, vector = tuple(line.split(\" \", 1))\n",
    "            glove_wordmap[name] = np.fromstring(vector, sep=\" \")\n",
    "\n",
    "    return glove_wordmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6DK1H0E4d3BB"
   },
   "outputs": [],
   "source": [
    "def sentence2sequence(sentence, wordmap, visualize=False):\n",
    "    \"\"\"\n",
    "    \n",
    "    Turns an input sentence into an (n,d) matrix, \n",
    "        where n is the number of tokens in the sentence\n",
    "        and d is the number of dimensions each word vector has.\n",
    "        \n",
    "    \"\"\"\n",
    "    tokens = sentence.lower().split(\" \")\n",
    "    rows = []\n",
    "    words = []\n",
    "    #Greedy search for tokens\n",
    "    for token in tokens:\n",
    "        i = len(token)\n",
    "        while len(token) > 0 and i > 0:\n",
    "            word = token[:i]\n",
    "            if word in wordmap:\n",
    "                rows.append(wordmap[word])\n",
    "                words.append(word)\n",
    "                token = token[i:]\n",
    "                i = len(token)\n",
    "            else:\n",
    "                i = i-1\n",
    "    \n",
    "    if visualize: return rows, words\n",
    "    else: return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZDvTpS_xeBGT"
   },
   "outputs": [],
   "source": [
    "def visualize(sentence, wordmap):\n",
    "    \"\"\"\n",
    "        Visualize GloVe Embeddings in a sentence\n",
    "    \"\"\"\n",
    "    rows, words = sentence2sequence(sentence, wordmap, visualize=True)\n",
    "    mat = np.vstack(rows)\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    shown = ax.matshow(mat, aspect=\"auto\")\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    fig.colorbar(shown)\n",
    "    \n",
    "    ax.set_yticklabels([\"\"]+words)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AEaU_7SQIycs"
   },
   "outputs": [],
   "source": [
    "def gen_csv(predicted, label_map, verbosity=False):\n",
    "    \"\"\"\n",
    "        Generate CSV with predicted results and\n",
    "        required form from the return of predict function and \n",
    "        the maping dictionnary {int: 'value'} \n",
    "    \"\"\"\n",
    "    import csv\n",
    "    from google.colab import files\n",
    "    predicted_results = np.argmax(predicted, axis=1)\n",
    "    if verbosity: print(label_map)\n",
    "    dict_data=[]\n",
    "    for i, v in enumerate(predicted_results): \n",
    "      d={'index':i, 'label':label_map[v]}\n",
    "      dict_data.append(d)\n",
    "    if verbosity: print(dict_data)\n",
    "    csv_file='results.csv'\n",
    "    try:\n",
    "        with open(csv_file, 'w') as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=['index', 'label'])\n",
    "            writer.writeheader()\n",
    "            for data in dict_data:\n",
    "                writer.writerow(data)\n",
    "    except IOError:\n",
    "        print(\"I/O error\")\n",
    "    files.download('results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q1ghnGPovBVY"
   },
   "outputs": [],
   "source": [
    "def data_from_csv(path, dev_mode=True, n_dev = 5000):\n",
    "    \"\"\" \n",
    "        retrieve data from CSV in a pandas.Dataframe\n",
    "    \"\"\"\n",
    "    # Entire dataset for training/validation\n",
    "    dataset = pd.read_csv(path, sep=\"\\t\")\n",
    "\n",
    "    # Dataset for dev\n",
    "    # less items to speed up computations\n",
    "    if dev_mode: \n",
    "      dataset = dataset.sample(n=n_dev)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hqHgaKN5ynCd"
   },
   "outputs": [],
   "source": [
    "def process_dataset(dataset, training=True, map_dict={'neutral':2, 'entailment':1, 'contradiction':0}):\n",
    "    \"\"\"\n",
    "        Return a preprocessed dataframe from the dataset sent in args\n",
    "        If the dataset doesn't have a label column set training to False\n",
    "    \"\"\"\n",
    "    import string # to get rid of the punctuation\n",
    "\n",
    "    n_dataset = dataset.copy()\n",
    "\n",
    "    n_dataset['sentence_1'] = [x.translate(str.maketrans('', '', string.punctuation)).lower()\n",
    "                             for x in dataset.sentence_1.values]\n",
    "\n",
    "    n_dataset['sentence_2'] = [x.translate(str.maketrans('', '', string.punctuation)).lower()\n",
    "                             for x in dataset.sentence_2.values]    \n",
    "\n",
    "    n_dataset['sentence_1'] = [np.array(sentence2sequence(x, glove_wordmap)) \\\n",
    "                             for x in dataset.sentence_1.values]\n",
    "\n",
    "    n_dataset['sentence_2'] = [np.array(sentence2sequence(x, glove_wordmap)) \\\n",
    "                             for x in dataset.sentence_2.values]\n",
    "\n",
    "    if training:\n",
    "        n_dataset['target'] = n_dataset['label'].replace(map_dict)\n",
    "\n",
    "    return n_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vaNLALzshyzq"
   },
   "source": [
    "## Model implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "9cIZ8S9FLQg9",
    "outputId": "4803e1c6-9257-4792-8e9b-9261365d964c"
   },
   "outputs": [],
   "source": [
    "# Import `Sequential` from `keras.models`\n",
    "from keras.models import Sequential\n",
    "# Import `Dense` from `keras.layers`\n",
    "import keras\n",
    "from keras.layers import Input, Dense, Dropout, Embedding, LSTM, Flatten, \\\n",
    "                          Bidirectional, Concatenate, GlobalAveragePooling1D\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8JnC2S4Uk84e"
   },
   "source": [
    "### Model 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dJVV4UORiDu4"
   },
   "source": [
    "#### Preprocess Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AW2jsNlWkuWm"
   },
   "outputs": [],
   "source": [
    "# Retrieve Embedded Vectors from Glove\n",
    "glove_wordmap = create_glove_wordmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ye2ZEmhb193g"
   },
   "outputs": [],
   "source": [
    "# Retrieve Dataset from CSV \n",
    "path_train=\"/content/drive/My Drive/MatMax-ING5/Deep Learning/projet_kaggle/data/dataset_train.csv\"\n",
    "path_test_no_label=\"/content/drive/My Drive/MatMax-ING5/Deep Learning/projet_kaggle/data/dataset_test_no_labels.csv\"\n",
    "\n",
    "dataset = data_from_csv(path_train, n_dev=5000, dev_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "UGYQa_az3NMj",
    "outputId": "6a487a07-acfe-4f44-c182-539e38cb5929"
   },
   "outputs": [],
   "source": [
    "map_dict={'neutral':2, 'entailment':1, 'contradiction':0}\n",
    "dataset_processed = process_dataset(dataset, map_dict=map_dict)\n",
    "print(\"Processed ! \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 200
    },
    "colab_type": "code",
    "id": "MKp_PadTDOsM",
    "outputId": "f883dc6e-c121-4ba3-a415-2834ad86cceb"
   },
   "outputs": [],
   "source": [
    "dataset_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tZQ1YAEpeW_S"
   },
   "outputs": [],
   "source": [
    "# On pad ici, sinon ca fait planter la RAM quand on utilise toutes les \n",
    "# lignes du CSV\n",
    "pad1 = pad_sequences(dataset_processed['sentence_1'].values, maxlen=80)\n",
    "pad2 = pad_sequences(dataset_processed['sentence_2'].values, maxlen=80)\n",
    "\n",
    "X = [pad1, pad2]\n",
    "y = dataset_processed['target'].values\n",
    "\n",
    "# (Nb inputs, nb words, nb dim)\n",
    "assert X[0][0].shape == (80, 50), \"Check the shape of your data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BZqf4e9RguCp"
   },
   "source": [
    "#### Model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 908
    },
    "colab_type": "code",
    "id": "U74diBpDfw9W",
    "outputId": "2137a405-f8ee-48af-e062-b5aeb4831260"
   },
   "outputs": [],
   "source": [
    "inputs1 = Input(shape=(80,50))\n",
    "inputs2 = Input(shape=(80,50))\n",
    "\n",
    "lstm_layer_1 = Bidirectional(LSTM(256, return_sequences=True))\n",
    "x1 = lstm_layer_1(inputs1)\n",
    "x2 = lstm_layer_1(inputs2)\n",
    "\n",
    "dropout_layer = Dropout(0.2)\n",
    "x1 = dropout_layer(x1)\n",
    "x2 = dropout_layer(x2)\n",
    "\n",
    "lstm_layer_2 = LSTM(128)\n",
    "x1 = lstm_layer_2(x1)\n",
    "x2 = lstm_layer_2(x2)\n",
    "\n",
    "dropout_layer = Dropout(0.2)\n",
    "x1 = dropout_layer(x1)\n",
    "x2 = dropout_layer(x2)\n",
    "\n",
    "dense = Dense(64, activation='relu')\n",
    "x1 = dense(x1)\n",
    "x2 = dense(x2)\n",
    "\n",
    "x1 = dropout_layer(x1)\n",
    "x2 = dropout_layer(x2)\n",
    "\n",
    "x = Concatenate(axis=-1)([x1,x2])\n",
    "predictions = Dense(3, activation='softmax')(x)\n",
    "\n",
    "model_1 = Model(inputs=[inputs1, inputs2], outputs=predictions)\n",
    "model_1.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hRXboyuNVOZz"
   },
   "outputs": [],
   "source": [
    "filepath=\"/content/drive/My Drive/MatMax-ING5/Deep Learning/projet_kaggle/data/weights-embedded_1.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 481
    },
    "colab_type": "code",
    "id": "GWqpNlV8jbFj",
    "outputId": "ec65705c-4dee-4445-9fdb-f2ba811c2f61"
   },
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "model_1.fit(X, to_categorical(y), epochs=1, batch_size=64, verbose=1, validation_split=0.25, callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 250
    },
    "colab_type": "code",
    "id": "6HoRl4p4KHno",
    "outputId": "a38e0e6c-8a3a-4fe6-c63b-0cc43e1fc83a"
   },
   "outputs": [],
   "source": [
    "model_1.load_weights(filepath, by_name=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v81jxLjoi3K_"
   },
   "source": [
    "#### Predict results and generate CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "colab_type": "code",
    "id": "D-UyfmaBjWy_",
    "outputId": "0e5bcfbc-e02a-4444-f333-fae5d181c0ed"
   },
   "outputs": [],
   "source": [
    "data_test = data_from_csv(path_test_no_label, dev_mode=False)\n",
    "\n",
    "data_test_processed = process_dataset(data_test, training=False)\n",
    "print(data_test_processed.head())\n",
    "X = [pad_sequences(data_test_processed['sentence_1'].values, maxlen=80), pad_sequences(data_test_processed['sentence_2'].values, maxlen=80)]\n",
    "\n",
    "assert X[0][0].shape == (80, 50), \"Check shape of your inputs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "colab_type": "code",
    "id": "AmQEFg_yjODn",
    "outputId": "40966ed9-ec4b-4e64-d6c3-bf4b69d13840"
   },
   "outputs": [],
   "source": [
    "predicted = model_1.predict(X)\n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Y78o1Y8jBNn"
   },
   "outputs": [],
   "source": [
    "map_dict = {v: k for k, v in map_dict.items()}\n",
    "gen_csv(predicted, map_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DhYE-SBwfvOW"
   },
   "source": [
    "### Model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oAbB_i_diWaC"
   },
   "source": [
    "#### Pre-process dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZEWR5-Zmk1yC"
   },
   "outputs": [],
   "source": [
    "# Retrieve Embedded Vectors from Glove\n",
    "glove_wordmap = create_glove_wordmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V0XXcwfdk1sa"
   },
   "outputs": [],
   "source": [
    "# Retrieve Dataset from CSV \n",
    "path_train=\"/content/drive/My Drive/MatMax-ING5/Deep Learning/projet_kaggle/data/dataset_train.csv\"\n",
    "path_test_no_label=\"/content/drive/My Drive/MatMax-ING5/Deep Learning/projet_kaggle/data/dataset_test_no_labels.csv\"\n",
    "\n",
    "dataset = data_from_csv(path_train, n_dev=5000, dev_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "etGjzDPpk1mL",
    "outputId": "2e078e9e-5b1e-436b-bb54-6edd5bebe85b"
   },
   "outputs": [],
   "source": [
    "map_dict={'neutral':2, 'entailment':1, 'contradiction':0}\n",
    "dataset_processed = process_dataset(dataset, map_dict=map_dict)\n",
    "print(\"Processed ! \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 200
    },
    "colab_type": "code",
    "id": "H7FQwRWOk1d_",
    "outputId": "85555e9a-b984-4a18-e5dd-866a17e62921"
   },
   "outputs": [],
   "source": [
    "dataset_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Oz4UdL9Nk1Ra"
   },
   "outputs": [],
   "source": [
    "# On pad ici, sinon ca fait planter la RAM quand on utilise toutes les \n",
    "# lignes du CSV\n",
    "X = [pad_sequences(dataset_processed['sentence_1'].values, maxlen=80), pad_sequences(dataset_processed['sentence_2'].values, maxlen=80)]\n",
    "y = dataset_processed['target'].values\n",
    "\n",
    "# (Nb inputs, nb words, nb dim)\n",
    "assert X[0][0].shape == (80, 50), \"Check the shape of your data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sTdbaLF4icXO"
   },
   "source": [
    "#### Model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 515
    },
    "colab_type": "code",
    "id": "jlcyFYhPfx3-",
    "outputId": "f1485559-bc4d-476c-c6d1-5a2f6068c0cc"
   },
   "outputs": [],
   "source": [
    "inputs1 = Input(shape=(80,50))\n",
    "inputs2 = Input(shape=(80,50))\n",
    "\n",
    "lstm_layer_1 = Bidirectional(LSTM(256))\n",
    "x1 = lstm_layer_1(inputs1)\n",
    "x2 = lstm_layer_1(inputs2)\n",
    "\n",
    "dropout_layer = Dropout(0.2)\n",
    "x1 = dropout_layer(x1)\n",
    "x2 = dropout_layer(x2)\n",
    "\n",
    "dense = Dense(64, activation='relu')\n",
    "x1 = dense(x1)\n",
    "x2 = dense(x2)\n",
    "\n",
    "x1 = dropout_layer(x1)\n",
    "x2 = dropout_layer(x2)\n",
    "\n",
    "x = Concatenate(axis=-1)([x1,x2])\n",
    "predictions = Dense(3, activation='softmax')(x)\n",
    "\n",
    "model_2 = Model(inputs=[inputs1, inputs2], outputs=predictions)\n",
    "model_2.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 266
    },
    "colab_type": "code",
    "id": "DAVXj3SHi5KU",
    "outputId": "c7f97207-7145-4b82-a0b1-f1e674313238"
   },
   "outputs": [],
   "source": [
    "filepath=\"/content/drive/My Drive/MatMax-ING5/Deep Learning/projet_kaggle/data/weights-embedded_2.hdf5\"\n",
    "checkpointer = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "model_2.fit(X, to_categorical(y), epochs=1, batch_size=64, verbose=1, validation_split=0.25, callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "baHFuZgiimdC"
   },
   "source": [
    "#### Predict results and generate CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qPwOXQGil3Rb"
   },
   "outputs": [],
   "source": [
    "data_test = data_from_csv(path_test_no_label, dev_mode=False)\n",
    "\n",
    "data_test_processed = process_dataset(data_test, training=False)\n",
    "\n",
    "X = [pad_sequences(data_test_processed['sentence_1'].values, maxlen=80), pad_sequences(data_test_processed['sentence_2'].values, maxlen=80)]\n",
    "\n",
    "assert X[0][0].shape == (80, 50), \"Check shape of your inputs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "colab_type": "code",
    "id": "XKJqZ1A6mAOh",
    "outputId": "fbd90ab7-da6e-4bfa-f370-2d03c48a1338"
   },
   "outputs": [],
   "source": [
    "predicted = model_2.predict(X)\n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ff-rOuJfi9TU"
   },
   "outputs": [],
   "source": [
    "map_dict = {v: k for k, v in map_dict.items()}\n",
    "gen_csv(predicted, map_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LTLa3kFbpBYe"
   },
   "source": [
    "### Model 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cOPg0ksMFwWr"
   },
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tw8wT36VpKVT"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import GlobalAveragePooling1D\n",
    "\n",
    "def preprocess_model_3(dataset, tokenizer,\n",
    "                       vocab_size = 50000, max_length = 80, \n",
    "                       training=True, \n",
    "                       map_dict={'neutral':2, 'entailment':1, 'contradiction':0}):\n",
    "    \"\"\"\n",
    "        Preprocess data for model_3\n",
    "    \"\"\"\n",
    "     \n",
    "    n_dataset = dataset.copy()\n",
    "    \n",
    "    if training:\n",
    "        tokenizer.fit_on_texts(dataset.sentence_1.values + dataset.sentence_2.values)\n",
    "\n",
    "    sentence1_seq = tokenizer.texts_to_sequences(n_dataset.sentence_1.values)\n",
    "    sentence2_seq = tokenizer.texts_to_sequences(n_dataset.sentence_2.values)\n",
    "    sentence2_seq_padded = pad_sequences(sentence2_seq, maxlen=max_length)\n",
    "    sentence1_seq_padded = pad_sequences(sentence1_seq, maxlen=max_length)\n",
    "\n",
    "    X = [sentence1_seq_padded,sentence2_seq_padded ]\n",
    "    \n",
    "    if training: \n",
    "        y = dataset['label'].replace(map_dict)\n",
    "        return X, y\n",
    "    else: \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QfezoetIobZj"
   },
   "outputs": [],
   "source": [
    "# Retrieve Dataset from CSV \n",
    "path_train=\"/content/drive/My Drive/MatMax-ING5/Deep Learning/projet_kaggle/data/dataset_train.csv\"\n",
    "path_test_no_label=\"/content/drive/My Drive/MatMax-ING5/Deep Learning/projet_kaggle/data/dataset_test_no_labels.csv\"\n",
    "\n",
    "dataset = data_from_csv(path_train, n_dev=5000, dev_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AlslzLHTdg40"
   },
   "outputs": [],
   "source": [
    "vocab_size = 50000\n",
    "map_dict = {'neutral':2, 'entailment':1, 'contradiction':0}\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "max_length = 80\n",
    "\n",
    "X, y = preprocess_model_3(dataset, tokenizer, max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PvkvGdTjotJe"
   },
   "outputs": [],
   "source": [
    "assert X[0][0].shape == (max_length,), \"Check your input\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sLN_7S0XF0hx"
   },
   "source": [
    "#### Model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 606
    },
    "colab_type": "code",
    "id": "XUHSWFIU3Uu1",
    "outputId": "1df50a84-085c-4f5c-de4b-abacc5ad9287"
   },
   "outputs": [],
   "source": [
    "inputs1 = Input(shape=(max_length,))\n",
    "inputs2 = Input(shape=(max_length,))\n",
    "\n",
    "embedding_layer = Embedding(vocab_size, 128, input_length=max_length)\n",
    "emb_out1 = embedding_layer(inputs1)\n",
    "emb_out2 = embedding_layer(inputs2)\n",
    "\n",
    "lstm_layer = LSTM(128)\n",
    "x1 = lstm_layer(emb_out1)\n",
    "x2 = lstm_layer(emb_out2)\n",
    "\n",
    "dropout_layer = Dropout(0.3)\n",
    "x1 = dropout_layer(x1)\n",
    "x2 = dropout_layer(x2)\n",
    "\n",
    "dense = Dense(64, activation='relu')\n",
    "x1 = dense(x1)\n",
    "x2 = dense(x2)\n",
    "\n",
    "dropout_layer = Dropout(0.2)\n",
    "x1 = dropout_layer(x1)\n",
    "x2 = dropout_layer(x2)\n",
    "\n",
    "x = Concatenate(axis=-1)([x1,x2])\n",
    "predictions = Dense(3, activation='softmax')(x)\n",
    "\n",
    "model_3 = Model(inputs=[inputs1, inputs2], outputs=predictions)\n",
    "model_3.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vd33OMODihHj"
   },
   "outputs": [],
   "source": [
    "filepath=\"/content/drive/My Drive/MatMax-ING5/Deep Learning/projet_kaggle/data/weights-embedded_3.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WhB1zhzh6EK5"
   },
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "model_3.fit(X, to_categorical(y), epochs=2, batch_size=64, verbose=1, validation_split=0.25, callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V17OzEOGicvn"
   },
   "outputs": [],
   "source": [
    "model_3.load_weights(filepath, by_name=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H_4IbE-CF38f"
   },
   "source": [
    "#### Predict results and generate CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gnZmLTw9Gtt7"
   },
   "outputs": [],
   "source": [
    "data_test = data_from_csv(path_test_no_label, dev_mode=False)\n",
    "\n",
    "X = preprocess_model_3(data_test, tokenizer, training=False)\n",
    "\n",
    "assert X[0][0].shape == (max_length,) and X[1][0].shape == (max_length,), \"Check X dimensions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "colab_type": "code",
    "id": "-J2AOwLSIBqR",
    "outputId": "463c1556-8e10-4458-b249-89734d9e0e3f"
   },
   "outputs": [],
   "source": [
    "predicted = model_3.predict(X)\n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QdoYPBlAI3Bf"
   },
   "outputs": [],
   "source": [
    "map_dict = {v: k for k, v in map_dict.items()}\n",
    "gen_csv(predicted, map_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AiarBjX4F4a1"
   },
   "source": [
    "### Model 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g1nUMFe6F4a4"
   },
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yJXqJBsbF4a4"
   },
   "outputs": [],
   "source": [
    "def preprocess_model_4(dataset, tokenizer,\n",
    "                       vocab_size = 50000, max_length = 80, \n",
    "                       training=True, \n",
    "                       map_dict={'neutral':2, 'entailment':1, 'contradiction':0}):\n",
    "    \"\"\"\n",
    "        Preprocess data for model_4\n",
    "    \"\"\"\n",
    "     \n",
    "    n_dataset = dataset.copy()\n",
    "    \n",
    "    if training:\n",
    "        tokenizer.fit_on_texts(dataset.sentence_1.values + dataset.sentence_2.values)\n",
    "\n",
    "    sentence1_seq = tokenizer.texts_to_sequences(n_dataset.sentence_1.values)\n",
    "    sentence2_seq = tokenizer.texts_to_sequences(n_dataset.sentence_2.values)\n",
    "    sentence2_seq_padded = pad_sequences(sentence2_seq, maxlen=max_length)\n",
    "    sentence1_seq_padded = pad_sequences(sentence1_seq, maxlen=max_length)\n",
    "\n",
    "    X = [sentence1_seq_padded,sentence2_seq_padded ]\n",
    "    \n",
    "    if training: \n",
    "        y = dataset['label'].replace(map_dict)\n",
    "        return X, y\n",
    "    else: \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WViqARMNF4a7"
   },
   "outputs": [],
   "source": [
    "# Retrieve Dataset from CSV \n",
    "path_train=\"/content/drive/My Drive/MatMax-ING5/Deep Learning/projet_kaggle/data/dataset_train.csv\"\n",
    "path_test_no_label=\"/content/drive/My Drive/MatMax-ING5/Deep Learning/projet_kaggle/data/dataset_test_no_labels.csv\"\n",
    "\n",
    "dataset = data_from_csv(path_train, n_dev=5000, dev_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r47fqm2EF4a8"
   },
   "outputs": [],
   "source": [
    "vocab_size = 50000\n",
    "map_dict = {'neutral':2, 'entailment':1, 'contradiction':0}\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "max_length = 80\n",
    "\n",
    "X, y = preprocess_model_4(dataset, tokenizer, max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NbXCENPhF4a-"
   },
   "outputs": [],
   "source": [
    "assert X[0][0].shape == (max_length,), \"Check your input\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tipVmW-HF4a_"
   },
   "source": [
    "#### Model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 586
    },
    "colab_type": "code",
    "id": "7Gz3VPKkF4bA",
    "outputId": "49759a40-0c86-4535-8d1a-3bd2a4ed8a00"
   },
   "outputs": [],
   "source": [
    "inputs1 = Input(shape=(max_length,))\n",
    "inputs2 = Input(shape=(max_length,))\n",
    "\n",
    "embedding_layer = Embedding(vocab_size, 128, input_length=max_length)\n",
    "emb_out1 = embedding_layer(inputs1)\n",
    "emb_out2 = embedding_layer(inputs2)\n",
    "\n",
    "lstm_layer = Bidirectional(LSTM(128))\n",
    "x1 = lstm_layer(emb_out1)\n",
    "x2 = lstm_layer(emb_out2)\n",
    "\n",
    "dropout_layer = Dropout(0.3)\n",
    "x1 = dropout_layer(x1)\n",
    "x2 = dropout_layer(x2)\n",
    "\n",
    "dense = Dense(64, activation='relu')\n",
    "x1 = dense(x1)\n",
    "x2 = dense(x2)\n",
    "\n",
    "dropout_layer = Dropout(0.2)\n",
    "x1 = dropout_layer(x1)\n",
    "x2 = dropout_layer(x2)\n",
    "\n",
    "x = Concatenate(axis=-1)([x1,x2])\n",
    "predictions = Dense(3, activation='softmax')(x)\n",
    "\n",
    "model_4 = Model(inputs=[inputs1, inputs2], outputs=predictions)\n",
    "model_4.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "model_4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HdmiTe7iF4bC"
   },
   "outputs": [],
   "source": [
    "filepath=\"/content/drive/My Drive/MatMax-ING5/Deep Learning/projet_kaggle/data/weights-embedded_4.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "id": "-Ylbj3WLF4bE",
    "outputId": "e7f475a2-fec2-4fe2-b32e-fb4719fb67d2"
   },
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "model_4.fit(X, to_categorical(y), epochs=2, batch_size=64, verbose=1, validation_split=0.25, callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tSu7kzLKF4bF"
   },
   "outputs": [],
   "source": [
    "model_4.load_weights(filepath, by_name=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PteQzLIMF4bH"
   },
   "source": [
    "#### Predict results and generate CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QnDLf8swF4bH"
   },
   "outputs": [],
   "source": [
    "data_test = data_from_csv(path_test_no_label, dev_mode=False)\n",
    "\n",
    "X = preprocess_model_3(data_test, tokenizer, training=False)\n",
    "\n",
    "assert X[0][0].shape == (max_length,) and X[1][0].shape == (max_length,), \"Check X dimensions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "colab_type": "code",
    "id": "WJqySQdHF4bJ",
    "outputId": "1f63335a-c66c-44a2-fcac-ea162b00a0a1"
   },
   "outputs": [],
   "source": [
    "predicted = model_4.predict(X)\n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MqKT_7-sF4bL"
   },
   "outputs": [],
   "source": [
    "map_dict = {v: k for k, v in map_dict.items()}\n",
    "gen_csv(predicted, map_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kYeCQMfSSQDD"
   },
   "source": [
    "### Model 5 - RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 464
    },
    "colab_type": "code",
    "id": "bY48QffdSQDB",
    "outputId": "2e05aaed-5066-4a82-91c8-a150d4d6e660"
   },
   "outputs": [],
   "source": [
    "!pip install fairseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "udwp6X3dSQC9"
   },
   "outputs": [],
   "source": [
    "# Retrieve Dataset from CSV \n",
    "path_train=\"/content/drive/My Drive/MatMax-ING5/Deep Learning/projet_kaggle/data/dataset_train.csv\"\n",
    "path_test_no_label=\"/content/drive/My Drive/MatMax-ING5/Deep Learning/projet_kaggle/data/dataset_test_no_labels.csv\"\n",
    "\n",
    "data_test = data_from_csv(path_test_no_label, n_dev=5000, dev_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "pWEYHyRtSQC6",
    "outputId": "d416f865-7f88-49fd-be4e-cefa977b6a38"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from fairseq.data.data_utils import collate_tokens\n",
    "\n",
    "roberta = torch.hub.load('pytorch/fairseq', 'roberta.large.mnli')\n",
    "roberta.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vw9h-u1ZSQC3"
   },
   "outputs": [],
   "source": [
    "batch_of_pairs = list()\n",
    "\n",
    "for i in range(int(len(data_test))):\n",
    "    batch_of_pairs.append([data_test.sentence_1[i], data_test.sentence_2[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "RHr5YNg6SQCw",
    "outputId": "c31e94c6-da72-4ba3-a10b-07c6dddeab6e"
   },
   "outputs": [],
   "source": [
    "predictions = list()\n",
    "with torch.no_grad():\n",
    "    # Encode a pair of sentences and make a prediction\n",
    "    for i, pair in enumerate(batch_of_pairs):\n",
    "      print(i, \"/\", len(batch_of_pairs))\n",
    "      tokens = roberta.encode(pair[0], pair[1])\n",
    "      prediction = roberta.predict('mnli', tokens).argmax().item()\n",
    "      predictions.append(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yVvv_K9lSQCq"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from google.colab import files\n",
    "label_map = {0: 'contradiction', 1: 'neutral', 2: 'entailment'}\n",
    "dict_data=[]\n",
    "\n",
    "for i, v in enumerate(predictions): \n",
    "    d={'index':i, 'label':label_map[v]}\n",
    "    dict_data.append(d)\n",
    "\n",
    "csv_file='results_roberta.csv'\n",
    "try:\n",
    "    with open(csv_file, 'w') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=['index', 'label'])\n",
    "        writer.writeheader()\n",
    "        for data in dict_data:\n",
    "            writer.writerow(data)\n",
    "\n",
    "except IOError:\n",
    "    print(\"I/O error\")\n",
    "files.download('results_roberta.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_oe6UIY8SQCa"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "8JnC2S4Uk84e",
    "BZqf4e9RguCp",
    "v81jxLjoi3K_",
    "DhYE-SBwfvOW",
    "oAbB_i_diWaC",
    "LTLa3kFbpBYe",
    "cOPg0ksMFwWr",
    "sLN_7S0XF0hx",
    "AiarBjX4F4a1"
   ],
   "machine_shape": "hm",
   "name": "ModelWithEmbeddings",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
